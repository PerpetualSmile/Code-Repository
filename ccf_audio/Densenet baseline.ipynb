{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T14:55:37.466142Z",
     "start_time": "2020-12-05T14:55:36.472119Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa as lb\n",
    "import pandas as pd\n",
    "import random\n",
    "import heapq\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from skimage.transform import resize\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T14:55:37.945666Z",
     "start_time": "2020-12-05T14:55:37.471141Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T14:55:37.953282Z",
     "start_time": "2020-12-05T14:55:37.947349Z"
    }
   },
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed) \n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "GLOBAL_SEED = 0\n",
    "setup_seed(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T14:55:37.974981Z",
     "start_time": "2020-12-05T14:55:37.956659Z"
    }
   },
   "outputs": [],
   "source": [
    "train_path = '/root/cfl/b/Audio/data/train'\n",
    "test_path = '/root/cfl/b/Audio/data/test'\n",
    "feat_path = '/root/cfl/b/Audio/features'\n",
    "res_path = '/root/cfl/b/Audio/res'\n",
    "model_save = '/root/cfl/b/Audio/model_save'\n",
    "tensorboard_path = '/root/cfl/b/Audio/tb_run'\n",
    "if not os.path.exists(model_save):\n",
    "    os.makedirs(model_save)\n",
    "if not os.path.exists(res_path):\n",
    "    os.makedirs(res_path)\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取特征保存为图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T14:55:38.004062Z",
     "start_time": "2020-12-05T14:55:37.978154Z"
    }
   },
   "outputs": [],
   "source": [
    "SR = 65535 * 2\n",
    "NUM_CLASSES = 30\n",
    "LABELS = os.listdir(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T14:55:38.037084Z",
     "start_time": "2020-12-05T14:55:38.010717Z"
    }
   },
   "outputs": [],
   "source": [
    "def mono_to_color(\n",
    "    X: np.ndarray, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6\n",
    "):\n",
    "    \"\"\"\n",
    "    Sources:\n",
    "        https://www.kaggle.com/daisukelab/creating-fat2019-preprocessed-data\n",
    "        https://www.kaggle.com/ttahara/training-birdsong-baseline-resnest50-fast\n",
    "    \"\"\"\n",
    "    # Stack X as [X,X,X]\n",
    "    X = np.stack([X, X, X], axis=-1)\n",
    "\n",
    "    # Standardize\n",
    "    mean = mean or X.mean()\n",
    "    X = X - mean\n",
    "    std = std or X.std()\n",
    "    Xstd = X / (std + eps)\n",
    "    _min, _max = Xstd.min(), Xstd.max()\n",
    "    norm_max = norm_max or _max\n",
    "    norm_min = norm_min or _min\n",
    "    if (_max - _min) > eps:\n",
    "        # Normalize to [0, 255]\n",
    "        V = Xstd\n",
    "        V[V < norm_min] = norm_min\n",
    "        V[V > norm_max] = norm_max\n",
    "        V = 255 * (V - norm_min) / (norm_max - norm_min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        # Just zero\n",
    "        V = np.zeros_like(Xstd, dtype=np.uint8)\n",
    "    return V\n",
    "\n",
    "\n",
    "def normalize(image, mean=None, std=None):\n",
    "    image = image / 255.0\n",
    "    if mean is not None and std is not None:\n",
    "        image = (image - mean) / std\n",
    "    return np.moveaxis(image, 2, 0).astype(np.float32)\n",
    "\n",
    "\n",
    "def get_melspec(x, sr, n_mels=256):\n",
    "    mel_spec = lb.feature.melspectrogram(x, sr=sr, n_mels=n_mels, n_fft=2048, hop_length=512, power=2)\n",
    "    mel_spec = lb.power_to_db(mel_spec).astype(np.float32)\n",
    "    image = mono_to_color(mel_spec)\n",
    "#     image = normalize(image, mean=None, std=None)\n",
    "    image = image.transpose(2, 1, 0)\n",
    "#     image = resize(image, (224, 400)) \n",
    "    return image\n",
    "\n",
    "\n",
    "def crop_or_pad(y, length):\n",
    "    if len(y) < length:\n",
    "        y = np.concatenate([y, np.zeros(length - len(y))])\n",
    "    elif len(y) > length:\n",
    "        y = y[: length]\n",
    "    return y\n",
    "\n",
    "def preprocess_train(train_path):\n",
    "    x, y = [], []\n",
    "    for i, label in enumerate(LABELS):\n",
    "        label_dir = os.path.join(train_path, label)\n",
    "        for wav_file in tqdm(os.listdir(label_dir)):\n",
    "            wav_path = os.path.join(train_path, label, wav_file)\n",
    "            wav, sr = lb.load(wav_path, sr=SR)\n",
    "            wav = crop_or_pad(wav, 1*SR)\n",
    "            melspec = get_melspec(wav, sr)\n",
    "            x.append(melspec)\n",
    "            y.append(i)\n",
    "\n",
    "    x, y = np.r_[x], np.r_[y]\n",
    "#     x, y = shuffle(x, y, random_state=GLOBAL_SEED)\n",
    "    return x, y.astype(np.int32)\n",
    "\n",
    "def preprocess_test(test_path):\n",
    "    x, keys = [], []\n",
    "\n",
    "    for wav_file in tqdm(os.listdir(test_path)):\n",
    "        wav_path = os.path.join(test_path, wav_file)\n",
    "        wav, sr = lb.load(wav_path, sr=SR)\n",
    "        wav = crop_or_pad(wav, 1*SR)\n",
    "        melspec = get_melspec(wav, sr)\n",
    "        x.append(melspec)\n",
    "        keys.append(wav_file)\n",
    "    x = np.r_[x]\n",
    "    return x, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-05T14:57:13.845Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 25/2122 [00:03<04:23,  7.96it/s]"
     ]
    }
   ],
   "source": [
    "X_train, y_train = preprocess_train(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T08:32:54.745181Z",
     "start_time": "2020-12-05T08:19:21.378182Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6835/6835 [13:30<00:00,  8.43it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test, test_names = preprocess_test(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T08:33:19.174317Z",
     "start_time": "2020-12-05T08:32:54.747568Z"
    }
   },
   "outputs": [],
   "source": [
    "np.savez(os.path.join(feat_path, 'melspec_256_256'), X=np.concatenate([X_train, X_test], axis=0), y=y_train, test_names=test_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载特征准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T14:55:46.955535Z",
     "start_time": "2020-12-05T14:55:46.948879Z"
    }
   },
   "outputs": [],
   "source": [
    "melspec = np.load(os.path.join(feat_path, 'melspec_256_256.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T14:56:02.600381Z",
     "start_time": "2020-12-05T14:55:47.307365Z"
    }
   },
   "outputs": [],
   "source": [
    "X = melspec['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T14:56:02.607345Z",
     "start_time": "2020-12-05T14:56:02.602229Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X[:57886] \n",
    "X_test = X[57886:]\n",
    "y_train = melspec['y']\n",
    "test_names = melspec['test_names']\n",
    "labels = os.listdir(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T14:56:02.694641Z",
     "start_time": "2020-12-05T14:56:02.609135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del melspec, X\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T14:56:02.710823Z",
     "start_time": "2020-12-05T14:56:02.696357Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, index, is_train=True):\n",
    "        self.index = index\n",
    "        self.is_train = is_train\n",
    "        self.transformer = transforms.Compose([\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_train:\n",
    "            img = X_train[self.index[idx]] / 255.0\n",
    "            return self.transformer(torch.tensor(img, dtype=torch.float32)), y_train[self.index[idx]]\n",
    "        else:\n",
    "            img = X_test[self.index[idx]] / 255.0\n",
    "            return self.transformer(torch.tensor(img, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T14:56:02.741803Z",
     "start_time": "2020-12-05T14:56:02.712387Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE_TRAIN = 64\n",
    "BATCH_SIZE_VAL = 64\n",
    "BATCH_SIZE_TEST = 64\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=GLOBAL_SEED)\n",
    "data_folds = []\n",
    "valid_indexs = []    \n",
    "\n",
    "\n",
    "test_dataset = CustomDataset(np.arange(X_test.shape[0]), is_train=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE_TEST, sampler=SequentialSampler(test_dataset), shuffle=False, num_workers=0)\n",
    "\n",
    "for idx, (train_index, valid_index) in enumerate(kf.split(X=X_train, y=y_train)):\n",
    "    valid_indexs.append(valid_index)\n",
    "    \n",
    "    train_dataset = CustomDataset(train_index, is_train=True)\n",
    "    val_dataset = CustomDataset(valid_index, is_train=True)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE_TRAIN, shuffle=True, num_workers=0)\n",
    "    valid_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE_VAL, sampler=SequentialSampler(val_dataset), shuffle=False, num_workers=0)\n",
    "    data_folds.append((train_dataloader, valid_dataloader, test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T14:56:02.746314Z",
     "start_time": "2020-12-05T14:56:02.743671Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T14:56:02.762040Z",
     "start_time": "2020-12-05T14:56:02.747947Z"
    }
   },
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = torchvision.models.densenet161(pretrained=True)\n",
    "        self.model.classifier = nn.Linear(2208, 30)    \n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T14:56:02.786495Z",
     "start_time": "2020-12-05T14:56:02.764376Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate(model, val_dataloader, criterion, history, n_iters):\n",
    "    model.eval()\n",
    "    costs = []\n",
    "    accs = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_dataloader):\n",
    "            X, y = batch\n",
    "            X, y = X.cuda(), y.cuda().long()\n",
    "            y_output = model(X)    \n",
    "            loss = criterion(y_output, y)\n",
    "            costs.append(loss.item())\n",
    "            _, y_preds = torch.max(y_output, 1)\n",
    "            accs.append((y_preds == y).float().mean().item())\n",
    "    mean_accs = np.mean(accs)\n",
    "    mean_costs = np.mean(costs)\n",
    "    writer.add_scalar('age/validate_accuracy', mean_accs, n_iters)\n",
    "    writer.add_scalar('age/validate_loss', mean_costs, n_iters)\n",
    "    if mean_accs > history['best_model'][0][0]:  \n",
    "        heapq.heapify(history['best_model'])\n",
    "        checkpoint_pth = history['best_model'][0][1]\n",
    "        heapq.heappushpop(history['best_model'], (mean_accs, checkpoint_pth))\n",
    "        torch.save(model.state_dict(), checkpoint_pth)\n",
    "    return mean_costs, mean_accs\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, criterion, optimizer, epoch, history, validate_points, scheduler, step=True):\n",
    "    model.train()\n",
    "    costs = []\n",
    "    accs = []\n",
    "    val_loss, val_acc = 0, 0\n",
    "    with tqdm(total=len(train_dataloader.dataset), desc='Epoch{}'.format(epoch)) as pbar:\n",
    "        for idx, batch in enumerate(train_dataloader):\n",
    "            X, y = batch\n",
    "            X, y = X.cuda(), y.cuda().long()\n",
    "            y_output = model(X)    \n",
    "            loss = criterion(y_output, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if step:\n",
    "                scheduler.step()\n",
    "            with torch.no_grad():\n",
    "                costs.append(loss.item())\n",
    "                _, y_preds = torch.max(y_output, 1)\n",
    "                accs.append((y_preds == y).float().mean().item())\n",
    "                pbar.update(y.size(0))\n",
    "            n_iters = idx + len(train_dataloader) * (epoch-1)\n",
    "            if idx in validate_points:\n",
    "                val_loss, val_acc = validate(model, val_dataloader, criterion, history, n_iters)\n",
    "                model.train()\n",
    "            \n",
    "            writer.add_scalar('age/train_accuracy', accs[-1], n_iters)\n",
    "            writer.add_scalar('age/train_loss', costs[-1], n_iters)\n",
    "            writer.add_scalar('age/learning_rate', scheduler.get_lr()[0], n_iters)\n",
    "            pbar.set_postfix_str('loss:{:.4f}, acc:{:.4f}, val-loss:{:.4f}, val-acc:{:.4f}'.format(np.mean(costs[-10:]), np.mean(accs[-10:]), val_loss, val_acc))\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    \n",
    "def test(oof_train_test, model, test_dataloader, val_dataloader, valid_index, weight=1):\n",
    "    model.eval()\n",
    "    y_preds = []\n",
    "    y_preds_val = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(test_dataloader):\n",
    "            X = batch\n",
    "            X= X.cuda()\n",
    "            y_output = model(X)    \n",
    "            y_preds.append(y_output.cpu())\n",
    "            \n",
    "        for idx, batch in enumerate(val_dataloader):\n",
    "            X, y = batch\n",
    "            X = X.cuda()\n",
    "            y_output = model(X)\n",
    "            y_preds_val.append(y_output.cpu())\n",
    "    \n",
    "    oof_train_test[valid_index] += F.softmax(torch.cat(y_preds_val)).numpy() * weight\n",
    "    oof_train_test[57886:] += F.softmax(torch.cat(y_preds)).numpy() * weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-05T14:57:40.623Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch1: 100%|██████████| 46308/46308 [13:53<00:00, 55.54it/s, loss:0.1006, acc:0.9738, val-loss:0.1083, val-acc:0.9702]  \n",
      "Epoch2: 100%|██████████| 46308/46308 [13:55<00:00, 55.46it/s, loss:0.0800, acc:0.9785, val-loss:0.1037, val-acc:0.9703]  \n",
      "Epoch3: 100%|██████████| 46308/46308 [13:55<00:00, 55.42it/s, loss:0.0725, acc:0.9792, val-loss:0.1012, val-acc:0.9719]  \n",
      "Epoch4: 100%|██████████| 46308/46308 [14:13<00:00, 54.26it/s, loss:0.0522, acc:0.9859, val-loss:0.1003, val-acc:0.9710]  \n",
      "Epoch5: 100%|██████████| 46308/46308 [15:14<00:00, 47.96it/s, loss:0.0811, acc:0.9750, val-loss:0.8000, val-acc:0.8251]  "
     ]
    }
   ],
   "source": [
    "def criterion(y_output, y_true):\n",
    "    loss = nn.CrossEntropyLoss()(y_output, y_true)\n",
    "    return loss\n",
    "\n",
    "res_folds = []\n",
    "acc_folds = []\n",
    "model_name = 'densenet161'\n",
    "best_checkpoint_num = 3\n",
    "for idx, (train_dataloader, val_dataloader, test_dataloader) in enumerate(data_folds):\n",
    "    oof_train_test = np.zeros((X_train.shape[0] + X_test.shape[0], 30))\n",
    "    history = {'best_model': []}\n",
    "    for i in range(best_checkpoint_num):\n",
    "        history['best_model'].append((0, os.path.join(model_save, '{}_checkpoint_fold_{}_{}.pth'.format(model_name, idx, i))))\n",
    "    validate_points = list(np.linspace(0, len(train_dataloader)-1, 3).astype(int))[1:]\n",
    "     \n",
    "    model = DenseNet().cuda()\n",
    "#     model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), lr=1e-3)\n",
    "    epochs = 6\n",
    "#     scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-5, max_lr=2e-3, step_size_up=int(len(train_dataloader)/2), cycle_momentum=False, mode='triangular')\n",
    "#     scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=3e-3, epochs=epochs, steps_per_epoch=len(train_dataloader), pct_start=0.2, anneal_strategy='linear', div_factor=30, final_div_factor=1e4)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        writer = SummaryWriter(log_dir=os.path.join(tensorboard_path, '{}_fold_{}'.format(model_name, idx)))\n",
    "        train(model, train_dataloader, val_dataloader, criterion, optimizer, epoch, history, validate_points, scheduler, step=True)\n",
    "#         scheduler.step()\n",
    "        gc.collect()\n",
    "    for (acc, checkpoint_pth), weight in zip(sorted(history['best_model'], reverse=True), [0.5, 0.3, 0.2]):\n",
    "        model.load_state_dict(torch.load(checkpoint_pth, map_location= torch.device('cpu')), strict=True)\n",
    "        test(oof_train_test, model, test_dataloader, val_dataloader, valid_indexs[idx], weight=weight)\n",
    "    acc_folds.append(sorted(history['best_model'], reverse=True)[0][0])\n",
    "    res_folds.append(oof_train_test)\n",
    "    np.save(os.path.join(res_path, \"{}_fold_{}.npy\".format(model_name, idx)), oof_train_test)\n",
    "    del model, history \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T02:11:45.429187Z",
     "start_time": "2020-12-06T02:11:45.421580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9746975615538286,\n",
       " 0.9758181278876836,\n",
       " 0.9734116022099447,\n",
       " 0.9724408137864171,\n",
       " 0.9727209944751382]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T02:11:05.982032Z",
     "start_time": "2020-12-06T02:11:05.944109Z"
    }
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(len(data_folds)):\n",
    "    res.append(np.load(os.path.join(res_path, \"{}_fold_{}.npy\".format(model_name, i))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T02:11:06.509676Z",
     "start_time": "2020-12-06T02:11:06.502691Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T02:11:08.430455Z",
     "start_time": "2020-12-06T02:11:08.367055Z"
    }
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['file_name'] = test_names\n",
    "sub['label'] = np.argmax(np.mean(res, axis=0)[57886:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T02:11:09.316614Z",
     "start_time": "2020-12-06T02:11:09.302994Z"
    }
   },
   "outputs": [],
   "source": [
    "sub['label'] = sub['label'].map({i:label for i, label in enumerate(labels)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T02:11:10.765457Z",
     "start_time": "2020-12-06T02:11:10.738495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3o9p4zffh0.wav</td>\n",
       "      <td>marvin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>srdw856mtq.wav</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>k42nwx43w4.wav</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6km36wy1rq.wav</td>\n",
       "      <td>five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mi8mrzrdra.wav</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6830</th>\n",
       "      <td>mk1xjjrsuv.wav</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6831</th>\n",
       "      <td>0ctd4hbh13.wav</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6832</th>\n",
       "      <td>akuoa16fdq.wav</td>\n",
       "      <td>stop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6833</th>\n",
       "      <td>vrjj8ay7x0.wav</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6834</th>\n",
       "      <td>957wa0sngr.wav</td>\n",
       "      <td>four</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6835 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           file_name   label\n",
       "0     3o9p4zffh0.wav  marvin\n",
       "1     srdw856mtq.wav   three\n",
       "2     k42nwx43w4.wav     yes\n",
       "3     6km36wy1rq.wav    five\n",
       "4     mi8mrzrdra.wav     two\n",
       "...              ...     ...\n",
       "6830  mk1xjjrsuv.wav   happy\n",
       "6831  0ctd4hbh13.wav     two\n",
       "6832  akuoa16fdq.wav    stop\n",
       "6833  vrjj8ay7x0.wav     cat\n",
       "6834  957wa0sngr.wav    four\n",
       "\n",
       "[6835 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T02:11:13.332893Z",
     "start_time": "2020-12-06T02:11:13.303544Z"
    }
   },
   "outputs": [],
   "source": [
    "now = time.strftime(\"%Y%m%d_%H%M%S\",time.localtime(time.time())) \n",
    "fname=\"submit_\" + model_name + \"_\" + now + \".csv\"    \n",
    "sub.to_csv(os.path.join(res_path, fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('py36_torch': conda)",
   "language": "python",
   "name": "python361064bitpy36torchcondaa3855d064b844135ba48c06bf4a0fbc1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
