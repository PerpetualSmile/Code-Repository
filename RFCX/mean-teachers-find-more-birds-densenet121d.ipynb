{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "I wanted to share something that worked pretty well for me early on in this competition. The idea comes from a [2018 paper](https://arxiv.org/pdf/1703.01780.pdf) titled *Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results* by Antti Tarvainen and Harri Valpola. \n",
    "\n",
    "### Mean Teacher\n",
    "Biefly, the idea is to use two models. A student model with weights trained the standard way, using backprop. And a teacher model with weights that are an exponential moving average of the student's weights. The teacher is the *mean* of the student \\*ba dum tss\\*. The student is then trained using two different losses, a standard classification loss and a consistency loss that penalizes student predictions that deviate from the teaher's. \n",
    "\n",
    "![](https://raw.githubusercontent.com/CuriousAI/mean-teacher/master/mean_teacher.png)\n",
    "\n",
    "Mean teachers are useful in a semi-supervised context where we have both labeled and unlabeled samples. The consistency loss on the unlabeled samples acts as a form of regularization and helps the model generalize better. As an added bonus the final teacher model is a temporal ensemble which tends to perform better than the results at the end of a single epoch. \n",
    "\n",
    "### Missing Labels\n",
    "As a few others have pointed out, there are a lot of missing labels. If we were to randomly sample a segment from the training data, we might consider it completely unlabeled rather than rely on the provided labels. We'll train our mean teacher model(s) on two classes of data, carefully selected positive samples and randomly selected unlabeled samples. The classification loss won't apply to the unlabeled samples. \n",
    "\n",
    "![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F4704212%2F9ca088bb386abf7114543c019c1d8a5f%2Ffig.png?generation=1609892974092435&alt=media)\n",
    "\n",
    "*Thanks to [shinmura0](https://www.kaggle.com/shinmurashinmura) for the great visualization!*\n",
    "\n",
    "### Results\n",
    "For me, mean teacher worked a good bit better than baseline models with similar configurations. \n",
    "\n",
    "|                                         | Baseline | Mean Teacher |\n",
    "|-----------------------------------------|----------|--------------|\n",
    "| Well Tuned, 5 fold, from my local setup | 0.847        | **0.865**            |\n",
    "| Single fold Expt1 on Kaggle                   | 0.592**        | **0.786**            |\n",
    "| Single fold Expt2 on Kaggle                   | 0.826        | **0.830**            |\n",
    "| 5 Fold on Kaggle***                        | 0.844        | **0.857**           |\n",
    "\n",
    "\\*\\* I might have accidentally sabatoged this run.\n",
    "\n",
    "\\*\\*\\* There was a major bug in v21 of the notebook where the consistence_ramp was set to 1000 which means it was just normal / non-mean-teacher training. Setting consisteny_ramp to 6 and using the mean teacher, we get an improvement of 0.13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T15:58:14.063301Z",
     "start_time": "2021-02-13T15:58:12.334698Z"
    },
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "import audiomentations as A\n",
    "import os, time, librosa, random\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchlibrosa.stft import Spectrogram, LogmelFilterBank\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "from contextlib import nullcontext\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from timm.models import resnet34d, resnet34, resnext50d_32x4d, densenet121\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T15:58:14.067740Z",
     "start_time": "2021-02-13T15:58:14.065120Z"
    }
   },
   "outputs": [],
   "source": [
    "# from resnest.torch import resnest50, resnest101, resnest200\n",
    "# from torchvision.models import resnet34, resnet50, resnet101, resnet152, densenet121, densenet169, densenet201, mobilenet_v2, resnext50_32x4d, resnext101_32x8d, wide_resnet50_2, wide_resnet101_2\n",
    "# from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T15:58:14.084827Z",
     "start_time": "2021-02-13T15:58:14.069314Z"
    }
   },
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "GLOBAL_SEED = 10\n",
    "setup_seed(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config\n",
    "We'll start by setting up some global config variable that we'll access later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T15:58:22.160605Z",
     "start_time": "2021-02-13T15:58:22.141980Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Global Vars\n",
    "NO_LABEL = -1\n",
    "NUM_CLASSES = 24\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "class config:\n",
    "    seed = GLOBAL_SEED\n",
    "    \n",
    "    train_tp_csv = '/dev/shm/data/train_tp.csv'\n",
    "    test_csv = '/dev/shm/data/sample_submission.csv'\n",
    "    save_path = '/root/s/RFCX/model_save'\n",
    "    res_path = '/root/s/RFCX/res'\n",
    "    train_data_path = \"/dev/shm/data/train\"\n",
    "    test_data_path = \"/dev/shm/data/test\"\n",
    "    tensorboard_path = '/root/s/RFCX/tensorboard'\n",
    "    model_name = \"densenet121-teacher\"\n",
    "    model = densenet121\n",
    "    \n",
    "    percent_unlabeled = 1.0\n",
    "    consistency_weight = 100.0\n",
    "    consistency_rampup = 6\n",
    "    ema_decay = 0.995\n",
    "    positive_weight = 3.0\n",
    "    \n",
    "    lr = 1e-3\n",
    "    epochs = 40\n",
    "    batch_size = 32\n",
    "    num_workers = 4\n",
    "    train_5_folds = True\n",
    "    \n",
    "    period = 8 # 6 second clips\n",
    "    step = 1\n",
    "    model_params = {\n",
    "        'sample_rate': 32000,\n",
    "        'window_size': 2048,\n",
    "        'hop_size': 512,\n",
    "        'mel_bins': 256,\n",
    "        'fmin': 20,\n",
    "        'fmax': 16000,\n",
    "        'classes_num': NUM_CLASSES\n",
    "    }\n",
    "    \n",
    "    augmenter = A.Compose([\n",
    "        A.AddGaussianNoise(p=0.33, max_amplitude=0.02),\n",
    "        A.AddGaussianSNR(p=0.33),\n",
    "        A.FrequencyMask(min_frequency_band=0.01,  max_frequency_band=0.25, p=0.33),\n",
    "        A.TimeMask(min_band_part=0.01, max_band_part=0.25, p=0.33),\n",
    "        A.Gain(p=0.33)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T15:58:22.606252Z",
     "start_time": "2021-02-13T15:58:22.561271Z"
    },
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "## Utils - Not much interesting going on here.\n",
    "\n",
    "def get_n_fold_df(csv_path, folds=5):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df_group = df.groupby(\"recording_id\")[[\"species_id\"]].first().reset_index()\n",
    "    df_group = df_group.sample(frac=1, random_state=config.seed).reset_index(drop=True)\n",
    "    df_group.loc[:, 'fold'] = -1\n",
    "\n",
    "    X = df_group[\"recording_id\"].values\n",
    "    y = df_group[\"species_id\"].values\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=folds, random_state=config.seed)\n",
    "    for fold, (t_idx, v_idx) in enumerate(kfold.split(X, y)):\n",
    "        df_group.loc[v_idx, \"fold\"] = fold\n",
    "\n",
    "    return df.merge(df_group[['recording_id', 'fold']], on=\"recording_id\", how=\"left\")\n",
    "    \n",
    "\n",
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "def sigmoid_rampup(current, rampup_length):\n",
    "    \"\"\"Exponential rampup from https://arxiv.org/abs/1610.02242\"\"\"\n",
    "    if rampup_length == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        current = np.clip(current, 0.0, rampup_length)\n",
    "        phase = 1.0 - current / rampup_length\n",
    "        return float(np.exp(-5.0 * phase * phase))\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class MetricMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.y_true = []\n",
    "        self.y_pred = []\n",
    "\n",
    "    def update(self, y_true, y_pred):\n",
    "        try:\n",
    "            self.y_true.extend(y_true.detach().cpu().numpy().tolist())\n",
    "            self.y_pred.extend(torch.sigmoid(y_pred).cpu().detach().numpy().tolist())\n",
    "        except:\n",
    "            print(\"UPDATE FAILURE\")\n",
    "\n",
    "    def update_list(self, y_true, y_pred):\n",
    "        self.y_true.extend(y_true)\n",
    "        self.y_pred.extend(y_pred)\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        score_class, weight = lwlrap(np.array(self.y_true), np.array(self.y_pred))\n",
    "        self.score = (score_class * weight).sum()\n",
    "\n",
    "        return self.score\n",
    "    \n",
    "\n",
    "def interpolate(x: torch.Tensor, ratio: int):\n",
    "    \"\"\"Interpolate data in time domain. This is used to compensate the\n",
    "    resolution reduction in downsampling of a CNN.\n",
    "\n",
    "    Args:\n",
    "      x: (batch_size, time_steps, classes_num)\n",
    "      ratio: int, ratio to interpolate\n",
    "    Returns:\n",
    "      upsampled: (batch_size, time_steps * ratio, classes_num)\n",
    "    \"\"\"\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "def _one_sample_positive_class_precisions(scores, truth):\n",
    "    num_classes = scores.shape[0]\n",
    "    pos_class_indices = np.flatnonzero(truth > 0)\n",
    "\n",
    "    if not len(pos_class_indices):\n",
    "        return pos_class_indices, np.zeros(0)\n",
    "\n",
    "    retrieved_classes = np.argsort(scores)[::-1]\n",
    "\n",
    "    class_rankings = np.zeros(num_classes, dtype=np.int)\n",
    "    class_rankings[retrieved_classes] = range(num_classes)\n",
    "\n",
    "    retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n",
    "    retrieved_class_true[class_rankings[pos_class_indices]] = True\n",
    "\n",
    "    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n",
    "\n",
    "    precision_at_hits = (\n",
    "            retrieved_cumulative_hits[class_rankings[pos_class_indices]] /\n",
    "            (1 + class_rankings[pos_class_indices].astype(np.float)))\n",
    "    return pos_class_indices, precision_at_hits\n",
    "\n",
    "\n",
    "def lwlrap(truth, scores):\n",
    "    assert truth.shape == scores.shape\n",
    "    num_samples, num_classes = scores.shape\n",
    "    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n",
    "    for sample_num in range(num_samples):\n",
    "        pos_class_indices, precision_at_hits = _one_sample_positive_class_precisions(scores[sample_num, :],\n",
    "                                                                                     truth[sample_num, :])\n",
    "        precisions_for_samples_by_classes[sample_num, pos_class_indices] = precision_at_hits\n",
    "\n",
    "    labels_per_class = np.sum(truth > 0, axis=0)\n",
    "    weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n",
    "\n",
    "    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) /\n",
    "                        np.maximum(1, labels_per_class))\n",
    "    return per_class_lwlrap, weight_per_class\n",
    "\n",
    "\n",
    "def pretty_print_metrics(fold, epoch, optimizer, train_loss_metrics, val_loss_metrics):\n",
    "    print(f\"\"\"\n",
    "    {time.ctime()} \\n\n",
    "    Fold:{fold}, Epoch:{epoch}, LR:{optimizer.param_groups[0]['lr']:.7}, Cons. Weight: {train_loss_metrics['consistency_weight']}\\n\n",
    "    --------------------------------------------------------\n",
    "    Metric:              Train    |   Val\n",
    "    --------------------------------------------------------\n",
    "    Loss:                {train_loss_metrics['loss']:0.4f}   |   {val_loss_metrics['loss']:0.4f}\\n\n",
    "    LWLRAP:              {train_loss_metrics['lwlrap']:0.4f}   |   {val_loss_metrics['lwlrap']:0.4f}\\n\n",
    "    Class Loss:          {train_loss_metrics['class_loss']:0.4f}   |   {val_loss_metrics['class_loss']:0.4f}\\n\n",
    "    Consistency Loss:    {train_loss_metrics['consistency_loss']:0.4f}   |   {val_loss_metrics['consistency_loss']:0.4f}\\n\n",
    "    --------------------------------------------------------\\n\n",
    "    \"\"\")\n",
    "    \n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, data_path, period=10, step=1):\n",
    "        self.data_path = data_path\n",
    "        self.period = period\n",
    "        self.step = step\n",
    "        self.recording_ids = list(df[\"recording_id\"].unique())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.recording_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        recording_id = self.recording_ids[idx]\n",
    "\n",
    "        y, sr = sf.read(f\"{self.data_path}/{recording_id}.wav\")\n",
    "\n",
    "        len_y = len(y)\n",
    "        effective_length = sr * self.period\n",
    "        effective_step = sr * self.step\n",
    "\n",
    "        y_ = []\n",
    "        i = 0\n",
    "        while i+effective_length <= len_y:\n",
    "            y__ = y[i:i + effective_length]\n",
    "            y_.append(y__)\n",
    "            i = i + effective_step\n",
    "\n",
    "        y = np.stack(y_)\n",
    "\n",
    "        label = np.zeros(NUM_CLASSES, dtype='f')\n",
    "\n",
    "        return {\n",
    "            \"waveform\": y,\n",
    "            \"target\": torch.tensor(label, dtype=torch.float),\n",
    "            \"id\": recording_id\n",
    "        }\n",
    "\n",
    "\n",
    "def predict_on_test(model, test_loader):\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    id_list = []\n",
    "    with torch.no_grad():\n",
    "        t = tqdm(test_loader)\n",
    "        for i, sample in enumerate(t):\n",
    "            input = sample[\"waveform\"].cuda()\n",
    "            bs, seq, w = input.shape\n",
    "            input = input.reshape(bs * seq, w)\n",
    "            id = sample[\"id\"]\n",
    "            output, _ = model(input)\n",
    "            output = output.reshape(bs, seq, -1)\n",
    "            output, _ = torch.max(output, dim=1)\n",
    "            \n",
    "            output = output.cpu().detach().numpy().tolist()\n",
    "            pred_list.extend(output)\n",
    "            id_list.extend(id)\n",
    "\n",
    "    return pred_list, id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "The model should look pretty familiar if you're using [SED](https://arxiv.org/abs/1912.04761). (Huge thanks to [Hidehisa Arai](https://www.kaggle.com/hidehisaarai1213) and their [SED Notebook](https://www.kaggle.com/hidehisaarai1213/introduction-to-sound-event-detection)!) You could use any model you'd like here. There's just one small tweak we need to make for our mean teacher setup. We need to \"detach\" the teacher's parameters so they aren't updated by the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T15:58:23.370015Z",
     "start_time": "2021-02-13T15:58:23.336646Z"
    }
   },
   "outputs": [],
   "source": [
    "class feature_extractor(nn.Module):\n",
    "    def __init__(self, original):\n",
    "        super().__init__()\n",
    "        self.model = original\n",
    "    def forward(self, x):\n",
    "        x= self.model.extract_features(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.conv_attention = nn.Conv1d(in_channels=in_features, \n",
    "                                        out_channels=out_features,\n",
    "                                        kernel_size=1, stride=1, \n",
    "                                        padding=0, bias=True)\n",
    "        self.conv_classes = nn.Conv1d(in_channels=in_features, \n",
    "                                      out_channels=out_features,\n",
    "                                      kernel_size=1, stride=1, \n",
    "                                      padding=0, bias=True)\n",
    "        self.batch_norm_attention = nn.BatchNorm1d(out_features)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.conv_attention)\n",
    "        init_layer(self.conv_classes)\n",
    "        init_bn(self.batch_norm_attention)\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm_att = torch.softmax(torch.tanh(self.conv_attention(x)), dim=-1)\n",
    "        classes = self.conv_classes(x)\n",
    "        x = torch.sum(norm_att * classes, dim=2)\n",
    "        return x, norm_att, classes\n",
    "\n",
    "\n",
    "class SEDAudioClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, sample_rate, window_size, hop_size, \n",
    "                 mel_bins, fmin, fmax, classes_num):\n",
    "        super().__init__()\n",
    "        self.interpolate_ratio = 32\n",
    "\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, \n",
    "                                                 hop_length=hop_size,\n",
    "                                                 win_length=window_size, \n",
    "                                                 window='hann', center=True,\n",
    "                                                 pad_mode='reflect', \n",
    "                                                 freeze_parameters=True)\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size,\n",
    "                                                 n_mels=mel_bins, fmin=fmin, \n",
    "                                                 fmax=fmax, ref=1.0, \n",
    "                                                 amin=1e-10, top_db=None, \n",
    "                                                 freeze_parameters=True)\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm2d(mel_bins)\n",
    "        \n",
    "        self.model = config.model(pretrained=True, in_chans=1)\n",
    "        if config.model_name.startswith('densenet'):\n",
    "            self.in_features = self.model.classifier.in_features\n",
    "        else:\n",
    "            self.in_features = self.model.fc.in_features\n",
    "        self.fc = nn.Linear(self.in_features, \n",
    "                            1024, bias=True)\n",
    "        self.att_head = AttentionHead(1024, classes_num)\n",
    "        self.avg_pool = nn.modules.pooling.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.batch_norm)\n",
    "        init_layer(self.fc)\n",
    "        self.att_head.init_weights()\n",
    "\n",
    "    def forward(self, input, spec_aug=False, \n",
    "                mixup_lambda=None, return_encoding=False):\n",
    "        x = self.spectrogram_extractor(input.float())\n",
    "        x = self.logmel_extractor(x)\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.batch_norm(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.model.forward_features(x)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_head(x)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        framewise_output = interpolate(segmentwise_output, self.interpolate_ratio)\n",
    "        return clipwise_output, framewise_output\n",
    "\n",
    "\n",
    "def get_model(is_mean_teacher=False):\n",
    "    model = SEDAudioClassifier(**config.model_params).cuda()\n",
    "    \n",
    "    # Detach params for Exponential Moving Average Model (aka the Mean Teacher).\n",
    "    # We'll manually update these params instead of using backprop.\n",
    "    if is_mean_teacher:\n",
    "        for param in model.parameters():\n",
    "            param.detach_()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "The loss function has 2 components:\n",
    "\n",
    "1. A classification loss that only applies to labeled samples.\n",
    "2. A consistency loss that applies to all samples. \n",
    "\n",
    "For the consistency loss we'll use the mean square error between the student and teacher predictions. We'll slowly ramp up the influence of the consistency loss since we don't want bad, early predictions having too much influence. \n",
    "\n",
    "Notice that we're weighting the positive samples for the classification loss. This is because we know the positives are correct while we're less sure about the negatives due to the missing labels issue. I found that this works better in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T15:58:24.411384Z",
     "start_time": "2021-02-13T15:58:24.389767Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_mse_loss(input_logits, target_logits):\n",
    "    assert input_logits.size() == target_logits.size()\n",
    "    input_softmax = torch.sigmoid(input_logits)\n",
    "    target_softmax = torch.sigmoid(target_logits)\n",
    "    num_classes = input_logits.size()[1]\n",
    "    return F.mse_loss(input_softmax, target_softmax, size_average=False\n",
    "                     ) / num_classes\n",
    "\n",
    "\n",
    "class MeanTeacherLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.positive_weight = torch.ones(NUM_CLASSES) * config.positive_weight\n",
    "        self.class_criterion = nn.BCEWithLogitsLoss(reduction='none', pos_weight=self.positive_weight)\n",
    "        self.consistency_criterion = sigmoid_mse_loss\n",
    "\n",
    "    def make_safe(self, pred):\n",
    "        pred = torch.where(torch.isnan(pred), torch.zeros_like(pred), pred)\n",
    "        return torch.where(torch.isinf(pred), torch.zeros_like(pred), pred)\n",
    "        \n",
    "    def get_consistency_weight(self, epoch):\n",
    "        # Consistency ramp-up from https://arxiv.org/abs/1610.02242\n",
    "        return config.consistency_weight * sigmoid_rampup(\n",
    "            epoch, config.consistency_rampup)\n",
    "    \n",
    "    def forward(self, student_pred, teacher_pred, target, classif_weights, epoch):\n",
    "        student_pred = self.make_safe(student_pred)\n",
    "        teacher_pred = self.make_safe(teacher_pred).detach().data\n",
    "\n",
    "        batch_size = len(target)\n",
    "        labeled_batch_size = target.ne(NO_LABEL).all(axis=1).sum().item() + 1e-3\n",
    "\n",
    "        student_classif, student_consistency = student_pred, student_pred\n",
    "        student_class_loss = (self.class_criterion(\n",
    "            student_classif, target) * classif_weights / labeled_batch_size).sum()\n",
    "\n",
    "        consistency_weights = self.get_consistency_weight(epoch)\n",
    "        consistency_loss = consistency_weights * self.consistency_criterion(\n",
    "            student_consistency, teacher_pred) / batch_size\n",
    "        loss = student_class_loss + consistency_loss\n",
    "        return loss, student_class_loss, consistency_loss, consistency_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader\n",
    "The data loader produces two types of samples:\n",
    "\n",
    "1. Labeled samples with the audio centered in the clip.\n",
    "2. Random unlabeled clips without labels selected from files with at least one true positive label.\n",
    "\n",
    "Each sample contains 2 different inputs, one for the student and one for the teacher. Different augmentations are applied to each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T15:58:25.095158Z",
     "start_time": "2021-02-13T15:58:25.056833Z"
    }
   },
   "outputs": [],
   "source": [
    "class MeanTeacherDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, transforms, period=5, \n",
    "                 data_path=config.train_data_path, \n",
    "                 val=False, percent_unlabeled=0.0):\n",
    "        self.period = period\n",
    "        self.transforms = transforms\n",
    "        self.data_path = data_path\n",
    "        self.val = val\n",
    "        self.percent_unlabeled = percent_unlabeled\n",
    "\n",
    "        dfgby = df.groupby(\"recording_id\").agg(lambda x: list(x)).reset_index()\n",
    "        self.recording_ids = dfgby[\"recording_id\"].values\n",
    "        self.species_ids = dfgby[\"species_id\"].values\n",
    "        self.t_mins = dfgby[\"t_min\"].values\n",
    "        self.t_maxs = dfgby[\"t_max\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.recording_ids) * (1 + self.percent_unlabeled))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.recording_ids):\n",
    "            audio, label, rec_id, sr = self.get_unlabeled_item(idx)\n",
    "            # For unlabeled samples, we zero out the classification loss.\n",
    "            classif_weights = np.zeros(NUM_CLASSES, dtype='f')\n",
    "        else:\n",
    "            audio, label, rec_id, sr = self.get_labeled_item(idx)\n",
    "            classif_weights = np.ones(NUM_CLASSES, dtype='f')\n",
    "\n",
    "        audio_teacher = np.copy(audio)\n",
    "\n",
    "        # The 2 samples fed to the 2 models have should have different augmentations.\n",
    "        audio = self.transforms(samples=audio, sample_rate=sr)\n",
    "        audio_teacher = self.transforms(samples=audio_teacher, sample_rate=sr)\n",
    "        # assert (audio != audio_teacher).any()\n",
    "        \n",
    "        return {\n",
    "            \"waveform\": audio,\n",
    "            \"teacher_waveform\": audio_teacher,\n",
    "            \"target\": torch.tensor(label, dtype=torch.float),\n",
    "            \"classification_weights\": classif_weights,\n",
    "            \"id\": rec_id\n",
    "        }\n",
    "\n",
    "    def get_labeled_item(self, idx):\n",
    "        recording_id = self.recording_ids[idx]\n",
    "        species_id = self.species_ids[idx]\n",
    "        t_min, t_max = self.t_mins[idx], self.t_maxs[idx]\n",
    "\n",
    "        rec, sr = sf.read(f\"{self.data_path}/{recording_id}.wav\")\n",
    "\n",
    "        len_rec = len(rec)\n",
    "        effective_length = sr * self.period\n",
    "        rint = np.random.randint(len(t_min))\n",
    "        tmin, tmax = round(sr * t_min[rint]), round(sr * t_max[rint])\n",
    "        dur = tmax - tmin\n",
    "        min_dur = min(dur, round(sr * self.period))\n",
    "\n",
    "        center = round((tmin + tmax) / 2)\n",
    "        rand_start = center - effective_length + max(min_dur - dur//2, 0)\n",
    "        if rand_start < 0:\n",
    "            rand_start = 0\n",
    "        rand_end = center - max(min_dur - dur//2, 0)\n",
    "        start = np.random.randint(rand_start, rand_end)\n",
    "        rec = rec[start:start + effective_length]\n",
    "        if len(rec) < effective_length:\n",
    "            new_rec = np.zeros(effective_length, dtype=rec.dtype)\n",
    "            start1 = np.random.randint(effective_length - len(rec))\n",
    "            new_rec[start1:start1 + len(rec)] = rec\n",
    "            rec = new_rec.astype(np.float32)\n",
    "        else:\n",
    "            rec = rec.astype(np.float32)\n",
    "\n",
    "        start_time = start / sr\n",
    "        end_time = (start + effective_length) / sr\n",
    "\n",
    "        label = np.zeros(NUM_CLASSES, dtype='f')\n",
    "\n",
    "        for i in range(len(t_min)):\n",
    "            if (t_min[i] >= start_time) & (t_max[i] <= end_time):\n",
    "                label[species_id[i]] = 1\n",
    "            elif start_time <= ((t_min[i] + t_max[i]) / 2) <= end_time:\n",
    "                label[species_id[i]] = 1\n",
    "\n",
    "        return rec, label, recording_id, sr\n",
    "\n",
    "    def get_unlabeled_item(self, idx, random_sample=False):\n",
    "        real_idx = idx - len(self.recording_ids)\n",
    "        # We want our validation set to be fixed.\n",
    "        if self.val:\n",
    "            rec_id = self.recording_ids[real_idx]\n",
    "        else:\n",
    "            rec_id = random.sample(list(self.recording_ids), 1)[0]\n",
    "\n",
    "        rec, sr = sf.read(f\"{self.data_path}/{rec_id}.wav\")\n",
    "        effective_length = int(sr * self.period)\n",
    "        max_end = len(rec) - effective_length\n",
    "        if self.val:\n",
    "            # Fixed start for validation. Probaably a better way to do this.\n",
    "            start = int(idx * 16963 % max_end)\n",
    "        else:\n",
    "            start = np.random.randint(0, max_end)\n",
    "        rec = rec[start:(start+effective_length)]\n",
    "        rec = rec.astype(np.float32)\n",
    "\n",
    "        label = np.ones(NUM_CLASSES, dtype='f') * NO_LABEL\n",
    "\n",
    "        return rec, label, rec_id, sr\n",
    "\n",
    "    \n",
    "def get_data_loader(df, is_val=False):\n",
    "    dataset = MeanTeacherDataset(\n",
    "        df=df,\n",
    "        transforms=config.augmenter,\n",
    "        period=config.period,\n",
    "        percent_unlabeled=config.percent_unlabeled\n",
    "    )\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=not is_val,\n",
    "        drop_last=not is_val,\n",
    "        num_workers=config.num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "At the end of each training step we update the teacher weights by averaging in the latest student weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T15:58:25.636852Z",
     "start_time": "2021-02-13T15:58:25.613590Z"
    }
   },
   "outputs": [],
   "source": [
    "# Update teacher to be exponential moving average of student params.\n",
    "def update_teacher_params(student, teacher, alpha, global_step):\n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (global_step + 1), alpha)\n",
    "    for ema_param, param in zip(teacher.parameters(), student.parameters()):\n",
    "        ema_param.data.mul_(alpha).add_(1 - alpha, param.data)\n",
    "\n",
    "\n",
    "def train_one_epoch(student, mean_teacher, loader, \n",
    "                    criterion, optimizer, scheduler, epoch, is_val=False):\n",
    "    global_step = 0\n",
    "    losses = AverageMeter()\n",
    "    consistency_loss_avg = AverageMeter()\n",
    "    class_loss_avg = AverageMeter()\n",
    "    comp_metric = MetricMeter()\n",
    "    \n",
    "    if is_val:\n",
    "        student.eval()\n",
    "        mean_teacher.eval()\n",
    "        context = torch.no_grad()\n",
    "    else:\n",
    "        student.train()\n",
    "        mean_teacher.train()\n",
    "        context = nullcontext()\n",
    "    \n",
    "    with context:\n",
    "        t = tqdm(loader)\n",
    "        for i, sample in enumerate(t):\n",
    "            student_input = sample['waveform'].cuda()\n",
    "            teacher_input = sample['teacher_waveform'].cuda()\n",
    "            target = sample['target'].cuda()\n",
    "            classif_weights = sample['classification_weights'].cuda()\n",
    "            batch_size = len(target)\n",
    "\n",
    "            student_pred, _  = student(student_input)\n",
    "            teacher_pred, _  = mean_teacher(teacher_input)\n",
    "\n",
    "            loss, class_loss, consistency_loss, consistency_weight = criterion(\n",
    "                student_pred, teacher_pred, target, classif_weights, epoch)\n",
    "\n",
    "            if not is_val:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                update_teacher_params(student, mean_teacher, \n",
    "                                      config.ema_decay, global_step)\n",
    "\n",
    "                scheduler.step()\n",
    "\n",
    "            comp_metric.update(target, student_pred)\n",
    "            losses.update(loss.item(), batch_size)\n",
    "            consistency_loss_avg.update(consistency_loss.item(), batch_size)\n",
    "            class_loss_avg.update(class_loss.item(), batch_size)\n",
    "            global_step += 1\n",
    "\n",
    "            t.set_description(f\"Epoch:{epoch} - Loss:{losses.avg:0.4f}\")\n",
    "        t.close()\n",
    "    return {'lwlrap':comp_metric.avg, \n",
    "            'loss':losses.avg, \n",
    "            'consistency_loss':consistency_loss_avg.avg, \n",
    "            'class_loss':class_loss_avg.avg, \n",
    "            'consistency_weight':consistency_weight}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally putting everything together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-13T15:58:26.300Z"
    },
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/densenet121_ra-50efcf5c.pth\" to /root/.cache/torch/hub/checkpoints/densenet121_ra-50efcf5c.pth\n",
      "Epoch:0 - Loss:14.5036: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:0 - Loss:10.5784: 100%|██████████| 14/14 [00:04<00:00,  3.02it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:01:59 2021 \n",
      "\n",
      "    Fold:0, Epoch:0, LR:7.669937e-05, Cons. Weight: 0.6737946999085467\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                14.5036   |   10.5784\n",
      "\n",
      "    LWLRAP:              0.1760   |   0.2212\n",
      "\n",
      "    Class Loss:          14.4482   |   10.5633\n",
      "\n",
      "    Consistency Loss:    0.0553   |   0.0151\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from -inf --> 0.2212203733374102\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 - Loss:10.4970: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:1 - Loss:8.9118: 100%|██████████| 14/14 [00:04<00:00,  3.18it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:02:34 2021 \n",
      "\n",
      "    Fold:0, Epoch:1, LR:0.0001811856, Cons. Weight: 3.1047958479329627\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                10.4970   |   8.9118\n",
      "\n",
      "    LWLRAP:              0.2124   |   0.3078\n",
      "\n",
      "    Class Loss:          10.4155   |   8.8688\n",
      "\n",
      "    Consistency Loss:    0.0815   |   0.0430\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.2212203733374102 --> 0.30784085104042924\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:2 - Loss:9.0896: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:2 - Loss:8.1459: 100%|██████████| 14/14 [00:04<00:00,  3.12it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:03:09 2021 \n",
      "\n",
      "    Fold:0, Epoch:2, LR:0.0003374814, Cons. Weight: 10.836802322189582\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                9.0896   |   8.1459\n",
      "\n",
      "    LWLRAP:              0.3209   |   0.4017\n",
      "\n",
      "    Class Loss:          8.8325   |   7.9929\n",
      "\n",
      "    Consistency Loss:    0.2571   |   0.1529\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.30784085104042924 --> 0.40167516058803876\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:3 - Loss:8.3922: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:3 - Loss:7.3820: 100%|██████████| 14/14 [00:04<00:00,  3.21it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:03:44 2021 \n",
      "\n",
      "    Fold:0, Epoch:3, LR:0.0005216868, Cons. Weight: 28.650479686019008\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                8.3922   |   7.3820\n",
      "\n",
      "    LWLRAP:              0.4112   |   0.5206\n",
      "\n",
      "    Class Loss:          7.8055   |   6.8553\n",
      "\n",
      "    Consistency Loss:    0.5867   |   0.5267\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.40167516058803876 --> 0.520587289584405\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:4 - Loss:7.5844: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:4 - Loss:6.8672: 100%|██████████| 14/14 [00:04<00:00,  3.12it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:04:19 2021 \n",
      "\n",
      "    Fold:0, Epoch:4, LR:0.0007056342, Cons. Weight: 57.375342073743276\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                7.5844   |   6.8672\n",
      "\n",
      "    LWLRAP:              0.5304   |   0.6357\n",
      "\n",
      "    Class Loss:          6.6143   |   5.9313\n",
      "\n",
      "    Consistency Loss:    0.9701   |   0.9359\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.520587289584405 --> 0.6357295221409716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:5 - Loss:6.5765: 100%|██████████| 56/56 [00:30<00:00,  1.87it/s]\n",
      "Epoch:5 - Loss:6.1757: 100%|██████████| 14/14 [00:04<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:04:53 2021 \n",
      "\n",
      "    Fold:0, Epoch:5, LR:0.0008611956, Cons. Weight: 87.03247258333906\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                6.5765   |   6.1757\n",
      "\n",
      "    LWLRAP:              0.6599   |   0.6936\n",
      "\n",
      "    Class Loss:          5.3253   |   4.8662\n",
      "\n",
      "    Consistency Loss:    1.2512   |   1.3095\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.6357295221409716 --> 0.6936470189978963\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:6 - Loss:5.9334: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:6 - Loss:5.8902: 100%|██████████| 14/14 [00:04<00:00,  3.22it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:05:28 2021 \n",
      "\n",
      "    Fold:0, Epoch:6, LR:0.0009645834, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                5.9334   |   5.8902\n",
      "\n",
      "    LWLRAP:              0.7187   |   0.7434\n",
      "\n",
      "    Class Loss:          4.6365   |   4.3414\n",
      "\n",
      "    Consistency Loss:    1.2970   |   1.5488\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.6936470189978963 --> 0.7434038479851006\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:7 - Loss:5.3950: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:7 - Loss:6.0759: 100%|██████████| 14/14 [00:04<00:00,  3.07it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:06:03 2021 \n",
      "\n",
      "    Fold:0, Epoch:7, LR:0.0009999992, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                5.3950   |   6.0759\n",
      "\n",
      "    LWLRAP:              0.7395   |   0.7603\n",
      "\n",
      "    Class Loss:          4.2165   |   4.3066\n",
      "\n",
      "    Consistency Loss:    1.1784   |   1.7693\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.7434038479851006 --> 0.760283210809797\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:8 - Loss:4.8909: 100%|██████████| 56/56 [00:30<00:00,  1.85it/s]\n",
      "Epoch:8 - Loss:5.3818: 100%|██████████| 14/14 [00:04<00:00,  3.15it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:06:38 2021 \n",
      "\n",
      "    Fold:0, Epoch:8, LR:0.0009975057, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                4.8909   |   5.3818\n",
      "\n",
      "    LWLRAP:              0.7818   |   0.7867\n",
      "\n",
      "    Class Loss:          3.7077   |   3.8335\n",
      "\n",
      "    Consistency Loss:    1.1832   |   1.5483\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.760283210809797 --> 0.7866831467602393\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:9 - Loss:4.4040: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:9 - Loss:4.7662: 100%|██████████| 14/14 [00:04<00:00,  3.15it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:07:13 2021 \n",
      "\n",
      "    Fold:0, Epoch:9, LR:0.0009902209, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                4.4040   |   4.7662\n",
      "\n",
      "    LWLRAP:              0.8236   |   0.8042\n",
      "\n",
      "    Class Loss:          3.3030   |   3.4850\n",
      "\n",
      "    Consistency Loss:    1.1010   |   1.2812\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.7866831467602393 --> 0.804202350111961\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:10 - Loss:3.8657: 100%|██████████| 56/56 [00:30<00:00,  1.85it/s]\n",
      "Epoch:10 - Loss:5.2758: 100%|██████████| 14/14 [00:04<00:00,  3.06it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:07:48 2021 \n",
      "\n",
      "    Fold:0, Epoch:10, LR:0.0009782151, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                3.8657   |   5.2758\n",
      "\n",
      "    LWLRAP:              0.8342   |   0.7956\n",
      "\n",
      "    Class Loss:          2.8796   |   4.0383\n",
      "\n",
      "    Consistency Loss:    0.9861   |   1.2375\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:11 - Loss:3.9101: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:11 - Loss:4.9435: 100%|██████████| 14/14 [00:04<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:08:23 2021 \n",
      "\n",
      "    Fold:0, Epoch:11, LR:0.0009616038, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                3.9101   |   4.9435\n",
      "\n",
      "    LWLRAP:              0.8488   |   0.8250\n",
      "\n",
      "    Class Loss:          2.8295   |   3.4987\n",
      "\n",
      "    Consistency Loss:    1.0806   |   1.4448\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.804202350111961 --> 0.8249842668344872\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:12 - Loss:3.4293: 100%|██████████| 56/56 [00:29<00:00,  1.87it/s]\n",
      "Epoch:12 - Loss:5.1043: 100%|██████████| 14/14 [00:04<00:00,  3.26it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:08:57 2021 \n",
      "\n",
      "    Fold:0, Epoch:12, LR:0.000940547, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                3.4293   |   5.1043\n",
      "\n",
      "    LWLRAP:              0.8634   |   0.7972\n",
      "\n",
      "    Class Loss:          2.4859   |   3.6687\n",
      "\n",
      "    Consistency Loss:    0.9433   |   1.4356\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:13 - Loss:3.4359: 100%|██████████| 56/56 [00:30<00:00,  1.84it/s]\n",
      "Epoch:13 - Loss:4.4113: 100%|██████████| 14/14 [00:04<00:00,  3.10it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:09:32 2021 \n",
      "\n",
      "    Fold:0, Epoch:13, LR:0.0009152475, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                3.4359   |   4.4113\n",
      "\n",
      "    LWLRAP:              0.8629   |   0.8326\n",
      "\n",
      "    Class Loss:          2.4507   |   3.4067\n",
      "\n",
      "    Consistency Loss:    0.9853   |   1.0046\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.8249842668344872 --> 0.8325659812543871\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:14 - Loss:3.0140: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:14 - Loss:5.2966: 100%|██████████| 14/14 [00:04<00:00,  3.14it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:10:07 2021 \n",
      "\n",
      "    Fold:0, Epoch:14, LR:0.000885949, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                3.0140   |   5.2966\n",
      "\n",
      "    LWLRAP:              0.8922   |   0.8117\n",
      "\n",
      "    Class Loss:          2.0897   |   3.9737\n",
      "\n",
      "    Consistency Loss:    0.9244   |   1.3229\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:15 - Loss:3.1196: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:15 - Loss:4.6319: 100%|██████████| 14/14 [00:04<00:00,  3.06it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:10:42 2021 \n",
      "\n",
      "    Fold:0, Epoch:15, LR:0.0008529336, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                3.1196   |   4.6319\n",
      "\n",
      "    LWLRAP:              0.8816   |   0.8086\n",
      "\n",
      "    Class Loss:          2.2261   |   3.4411\n",
      "\n",
      "    Consistency Loss:    0.8936   |   1.1908\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:16 - Loss:2.6369: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:16 - Loss:4.6409: 100%|██████████| 14/14 [00:04<00:00,  3.20it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:11:17 2021 \n",
      "\n",
      "    Fold:0, Epoch:16, LR:0.0008165193, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                2.6369   |   4.6409\n",
      "\n",
      "    LWLRAP:              0.9032   |   0.8040\n",
      "\n",
      "    Class Loss:          1.7884   |   3.4709\n",
      "\n",
      "    Consistency Loss:    0.8485   |   1.1699\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:17 - Loss:2.7288: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:17 - Loss:5.7599: 100%|██████████| 14/14 [00:04<00:00,  3.06it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:11:51 2021 \n",
      "\n",
      "    Fold:0, Epoch:17, LR:0.0007770567, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                2.7288   |   5.7599\n",
      "\n",
      "    LWLRAP:              0.9076   |   0.7989\n",
      "\n",
      "    Class Loss:          1.8141   |   4.3494\n",
      "\n",
      "    Consistency Loss:    0.9147   |   1.4105\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:18 - Loss:2.5437: 100%|██████████| 56/56 [00:30<00:00,  1.87it/s]\n",
      "Epoch:18 - Loss:4.3165: 100%|██████████| 14/14 [00:04<00:00,  3.22it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:12:26 2021 \n",
      "\n",
      "    Fold:0, Epoch:18, LR:0.000734926, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                2.5437   |   4.3165\n",
      "\n",
      "    LWLRAP:              0.9144   |   0.8424\n",
      "\n",
      "    Class Loss:          1.7109   |   3.1636\n",
      "\n",
      "    Consistency Loss:    0.8328   |   1.1529\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.8325659812543871 --> 0.8424419370077845\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:19 - Loss:2.7906: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:19 - Loss:4.8063: 100%|██████████| 14/14 [00:04<00:00,  3.16it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:13:01 2021 \n",
      "\n",
      "    Fold:0, Epoch:19, LR:0.0006905328, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                2.7906   |   4.8063\n",
      "\n",
      "    LWLRAP:              0.9071   |   0.8308\n",
      "\n",
      "    Class Loss:          1.9137   |   3.6256\n",
      "\n",
      "    Consistency Loss:    0.8769   |   1.1806\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:20 - Loss:2.1940: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:20 - Loss:4.0270: 100%|██████████| 14/14 [00:04<00:00,  3.08it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:13:35 2021 \n",
      "\n",
      "    Fold:0, Epoch:20, LR:0.0006443047, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                2.1940   |   4.0270\n",
      "\n",
      "    LWLRAP:              0.9255   |   0.8374\n",
      "\n",
      "    Class Loss:          1.4549   |   3.0663\n",
      "\n",
      "    Consistency Loss:    0.7392   |   0.9606\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:21 - Loss:2.0450: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:21 - Loss:3.9722: 100%|██████████| 14/14 [00:04<00:00,  3.09it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:14:10 2021 \n",
      "\n",
      "    Fold:0, Epoch:21, LR:0.0005966869, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                2.0450   |   3.9722\n",
      "\n",
      "    LWLRAP:              0.9330   |   0.8425\n",
      "\n",
      "    Class Loss:          1.3063   |   3.0535\n",
      "\n",
      "    Consistency Loss:    0.7387   |   0.9188\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.8424419370077845 --> 0.8424838945108336\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:22 - Loss:1.7801: 100%|██████████| 56/56 [00:30<00:00,  1.84it/s]\n",
      "Epoch:22 - Loss:4.5900: 100%|██████████| 14/14 [00:04<00:00,  3.14it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:14:45 2021 \n",
      "\n",
      "    Fold:0, Epoch:22, LR:0.000548138, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.7801   |   4.5900\n",
      "\n",
      "    LWLRAP:              0.9448   |   0.8226\n",
      "\n",
      "    Class Loss:          1.1292   |   3.5806\n",
      "\n",
      "    Consistency Loss:    0.6510   |   1.0093\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:23 - Loss:1.8173: 100%|██████████| 56/56 [00:30<00:00,  1.85it/s]\n",
      "Epoch:23 - Loss:4.2262: 100%|██████████| 14/14 [00:04<00:00,  3.14it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:15:20 2021 \n",
      "\n",
      "    Fold:0, Epoch:23, LR:0.0004991254, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.8173   |   4.2262\n",
      "\n",
      "    LWLRAP:              0.9367   |   0.8299\n",
      "\n",
      "    Class Loss:          1.1330   |   3.3697\n",
      "\n",
      "    Consistency Loss:    0.6843   |   0.8566\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:24 - Loss:1.8325: 100%|██████████| 56/56 [00:30<00:00,  1.85it/s]\n",
      "Epoch:24 - Loss:4.0908: 100%|██████████| 14/14 [00:04<00:00,  3.18it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:15:55 2021 \n",
      "\n",
      "    Fold:0, Epoch:24, LR:0.0004501214, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.8325   |   4.0908\n",
      "\n",
      "    LWLRAP:              0.9452   |   0.8382\n",
      "\n",
      "    Class Loss:          1.1717   |   3.2999\n",
      "\n",
      "    Consistency Loss:    0.6608   |   0.7909\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:25 - Loss:1.6239: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:25 - Loss:4.3682: 100%|██████████| 14/14 [00:04<00:00,  3.10it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:16:30 2021 \n",
      "\n",
      "    Fold:0, Epoch:25, LR:0.0004015977, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.6239   |   4.3682\n",
      "\n",
      "    LWLRAP:              0.9539   |   0.8326\n",
      "\n",
      "    Class Loss:          0.9966   |   3.4648\n",
      "\n",
      "    Consistency Loss:    0.6273   |   0.9034\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:26 - Loss:1.4095: 100%|██████████| 56/56 [00:30<00:00,  1.85it/s]\n",
      "Epoch:26 - Loss:3.7348: 100%|██████████| 14/14 [00:04<00:00,  3.13it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:17:05 2021 \n",
      "\n",
      "    Fold:0, Epoch:26, LR:0.0003540217, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.4095   |   3.7348\n",
      "\n",
      "    LWLRAP:              0.9581   |   0.8656\n",
      "\n",
      "    Class Loss:          0.8026   |   2.9191\n",
      "\n",
      "    Consistency Loss:    0.6069   |   0.8157\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.8424838945108336 --> 0.8656460917730684\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:27 - Loss:1.4517: 100%|██████████| 56/56 [00:30<00:00,  1.85it/s]\n",
      "Epoch:27 - Loss:4.3264: 100%|██████████| 14/14 [00:04<00:00,  3.20it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:17:40 2021 \n",
      "\n",
      "    Fold:0, Epoch:27, LR:0.0003078515, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.4517   |   4.3264\n",
      "\n",
      "    LWLRAP:              0.9582   |   0.8297\n",
      "\n",
      "    Class Loss:          0.8057   |   3.4419\n",
      "\n",
      "    Consistency Loss:    0.6460   |   0.8846\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:28 - Loss:1.3813: 100%|██████████| 56/56 [00:30<00:00,  1.87it/s]\n",
      "Epoch:28 - Loss:3.5410: 100%|██████████| 14/14 [00:04<00:00,  3.04it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:18:14 2021 \n",
      "\n",
      "    Fold:0, Epoch:28, LR:0.0002635319, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.3813   |   3.5410\n",
      "\n",
      "    LWLRAP:              0.9620   |   0.8574\n",
      "\n",
      "    Class Loss:          0.8085   |   2.8231\n",
      "\n",
      "    Consistency Loss:    0.5727   |   0.7179\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:29 - Loss:1.2818: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:29 - Loss:3.7229: 100%|██████████| 14/14 [00:04<00:00,  3.19it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:18:49 2021 \n",
      "\n",
      "    Fold:0, Epoch:29, LR:0.0002214896, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.2818   |   3.7229\n",
      "\n",
      "    LWLRAP:              0.9671   |   0.8549\n",
      "\n",
      "    Class Loss:          0.7273   |   3.0361\n",
      "\n",
      "    Consistency Loss:    0.5545   |   0.6868\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:30 - Loss:1.2338: 100%|██████████| 56/56 [00:29<00:00,  1.87it/s]\n",
      "Epoch:30 - Loss:4.1070: 100%|██████████| 14/14 [00:04<00:00,  3.05it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:19:24 2021 \n",
      "\n",
      "    Fold:0, Epoch:30, LR:0.0001821295, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.2338   |   4.1070\n",
      "\n",
      "    LWLRAP:              0.9655   |   0.8458\n",
      "\n",
      "    Class Loss:          0.7312   |   3.3359\n",
      "\n",
      "    Consistency Loss:    0.5026   |   0.7712\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:31 - Loss:1.0813: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:31 - Loss:3.8815: 100%|██████████| 14/14 [00:04<00:00,  3.19it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:19:58 2021 \n",
      "\n",
      "    Fold:0, Epoch:31, LR:0.0001458307, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.0813   |   3.8815\n",
      "\n",
      "    LWLRAP:              0.9722   |   0.8365\n",
      "\n",
      "    Class Loss:          0.6166   |   3.1893\n",
      "\n",
      "    Consistency Loss:    0.4647   |   0.6923\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:32 - Loss:1.1262: 100%|██████████| 56/56 [00:30<00:00,  1.84it/s]\n",
      "Epoch:32 - Loss:4.0720: 100%|██████████| 14/14 [00:04<00:00,  3.01it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:20:33 2021 \n",
      "\n",
      "    Fold:0, Epoch:32, LR:0.0001129428, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.1262   |   4.0720\n",
      "\n",
      "    LWLRAP:              0.9736   |   0.8328\n",
      "\n",
      "    Class Loss:          0.5990   |   3.4242\n",
      "\n",
      "    Consistency Loss:    0.5272   |   0.6478\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:33 - Loss:1.0130: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:33 - Loss:4.0810: 100%|██████████| 14/14 [00:04<00:00,  3.22it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:21:08 2021 \n",
      "\n",
      "    Fold:0, Epoch:33, LR:8.378251e-05, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.0130   |   4.0810\n",
      "\n",
      "    LWLRAP:              0.9758   |   0.8409\n",
      "\n",
      "    Class Loss:          0.5216   |   3.4756\n",
      "\n",
      "    Consistency Loss:    0.4915   |   0.6053\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:34 - Loss:1.2001: 100%|██████████| 56/56 [00:30<00:00,  1.85it/s]\n",
      "Epoch:34 - Loss:3.6795: 100%|██████████| 14/14 [00:04<00:00,  3.09it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:21:43 2021 \n",
      "\n",
      "    Fold:0, Epoch:34, LR:5.86306e-05, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.2001   |   3.6795\n",
      "\n",
      "    LWLRAP:              0.9626   |   0.8743\n",
      "\n",
      "    Class Loss:          0.6895   |   3.1121\n",
      "\n",
      "    Consistency Loss:    0.5106   |   0.5674\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.8656460917730684 --> 0.8743407743214244\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:35 - Loss:1.0787: 100%|██████████| 56/56 [00:30<00:00,  1.87it/s]\n",
      "Epoch:35 - Loss:3.8851: 100%|██████████| 14/14 [00:04<00:00,  3.07it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:22:18 2021 \n",
      "\n",
      "    Fold:0, Epoch:35, LR:3.772935e-05, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.0787   |   3.8851\n",
      "\n",
      "    LWLRAP:              0.9736   |   0.8486\n",
      "\n",
      "    Class Loss:          0.5869   |   3.1201\n",
      "\n",
      "    Consistency Loss:    0.4918   |   0.7650\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:36 - Loss:1.1218: 100%|██████████| 56/56 [00:29<00:00,  1.87it/s]\n",
      "Epoch:36 - Loss:3.4808: 100%|██████████| 14/14 [00:04<00:00,  3.24it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:22:52 2021 \n",
      "\n",
      "    Fold:0, Epoch:36, LR:2.128003e-05, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.1218   |   3.4808\n",
      "\n",
      "    LWLRAP:              0.9710   |   0.8611\n",
      "\n",
      "    Class Loss:          0.6389   |   2.8351\n",
      "\n",
      "    Consistency Loss:    0.4829   |   0.6457\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:37 - Loss:0.9338: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:37 - Loss:3.6722: 100%|██████████| 14/14 [00:04<00:00,  3.13it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:23:27 2021 \n",
      "\n",
      "    Fold:0, Epoch:37, LR:9.441067e-06, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                0.9338   |   3.6722\n",
      "\n",
      "    LWLRAP:              0.9773   |   0.8556\n",
      "\n",
      "    Class Loss:          0.4826   |   2.9849\n",
      "\n",
      "    Consistency Loss:    0.4512   |   0.6873\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:38 - Loss:0.9430: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:38 - Loss:3.7942: 100%|██████████| 14/14 [00:04<00:00,  3.15it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:24:01 2021 \n",
      "\n",
      "    Fold:0, Epoch:38, LR:2.326474e-06, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                0.9430   |   3.7942\n",
      "\n",
      "    LWLRAP:              0.9794   |   0.8496\n",
      "\n",
      "    Class Loss:          0.4779   |   3.1905\n",
      "\n",
      "    Consistency Loss:    0.4651   |   0.6036\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:39 - Loss:1.0561: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:39 - Loss:3.8480: 100%|██████████| 14/14 [00:04<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:24:36 2021 \n",
      "\n",
      "    Fold:0, Epoch:39, LR:4.768355e-09, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.0561   |   3.8480\n",
      "\n",
      "    LWLRAP:              0.9737   |   0.8558\n",
      "\n",
      "    Class Loss:          0.5787   |   3.2287\n",
      "\n",
      "    Consistency Loss:    0.4774   |   0.6193\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:0 - Loss:13.2505: 100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n",
      "Epoch:0 - Loss:9.8836: 100%|██████████| 14/14 [00:04<00:00,  3.19it/s] \n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:25:15 2021 \n",
      "\n",
      "    Fold:1, Epoch:0, LR:7.669937e-05, Cons. Weight: 0.6737946999085467\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                13.2505   |   9.8836\n",
      "\n",
      "    LWLRAP:              0.1568   |   0.2149\n",
      "\n",
      "    Class Loss:          13.2070   |   9.8718\n",
      "\n",
      "    Consistency Loss:    0.0435   |   0.0119\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from -inf --> 0.21490962606489594\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 - Loss:10.4105: 100%|██████████| 56/56 [00:29<00:00,  1.87it/s]\n",
      "Epoch:1 - Loss:9.2928: 100%|██████████| 14/14 [00:04<00:00,  3.17it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:25:50 2021 \n",
      "\n",
      "    Fold:1, Epoch:1, LR:0.0001811856, Cons. Weight: 3.1047958479329627\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                10.4105   |   9.2928\n",
      "\n",
      "    LWLRAP:              0.2254   |   0.2534\n",
      "\n",
      "    Class Loss:          10.3244   |   9.2448\n",
      "\n",
      "    Consistency Loss:    0.0862   |   0.0480\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.21490962606489594 --> 0.25337313771533826\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:2 - Loss:9.4091: 100%|██████████| 56/56 [00:30<00:00,  1.85it/s] \n",
      "Epoch:2 - Loss:8.3727: 100%|██████████| 14/14 [00:04<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:26:25 2021 \n",
      "\n",
      "    Fold:1, Epoch:2, LR:0.0003374814, Cons. Weight: 10.836802322189582\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                9.4091   |   8.3727\n",
      "\n",
      "    LWLRAP:              0.2879   |   0.3719\n",
      "\n",
      "    Class Loss:          9.1582   |   8.1746\n",
      "\n",
      "    Consistency Loss:    0.2509   |   0.1980\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.25337313771533826 --> 0.3719296644709902\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:3 - Loss:8.3112: 100%|██████████| 56/56 [00:54<00:00,  1.02it/s]\n",
      "Epoch:3 - Loss:8.2104: 100%|██████████| 14/14 [00:08<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:27:28 2021 \n",
      "\n",
      "    Fold:1, Epoch:3, LR:0.0005216868, Cons. Weight: 28.650479686019008\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                8.3112   |   8.2104\n",
      "\n",
      "    LWLRAP:              0.4268   |   0.4914\n",
      "\n",
      "    Class Loss:          7.7517   |   7.5470\n",
      "\n",
      "    Consistency Loss:    0.5595   |   0.6634\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.3719296644709902 --> 0.4913750661103602\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:4 - Loss:7.5719: 100%|██████████| 56/56 [00:57<00:00,  1.02s/it]\n",
      "Epoch:4 - Loss:7.0433: 100%|██████████| 14/14 [00:08<00:00,  1.72it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:28:34 2021 \n",
      "\n",
      "    Fold:1, Epoch:4, LR:0.0007056342, Cons. Weight: 57.375342073743276\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                7.5719   |   7.0433\n",
      "\n",
      "    LWLRAP:              0.5380   |   0.5828\n",
      "\n",
      "    Class Loss:          6.6278   |   6.0833\n",
      "\n",
      "    Consistency Loss:    0.9441   |   0.9601\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.4913750661103602 --> 0.5827901063753803\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:5 - Loss:6.5933: 100%|██████████| 56/56 [00:56<00:00,  1.00s/it]\n",
      "Epoch:5 - Loss:6.5576: 100%|██████████| 14/14 [00:08<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:29:39 2021 \n",
      "\n",
      "    Fold:1, Epoch:5, LR:0.0008611956, Cons. Weight: 87.03247258333906\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                6.5933   |   6.5576\n",
      "\n",
      "    LWLRAP:              0.6388   |   0.6947\n",
      "\n",
      "    Class Loss:          5.3768   |   5.0049\n",
      "\n",
      "    Consistency Loss:    1.2165   |   1.5526\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.5827901063753803 --> 0.694725555213344\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:6 - Loss:6.0863: 100%|██████████| 56/56 [00:55<00:00,  1.00it/s]\n",
      "Epoch:6 - Loss:6.6991: 100%|██████████| 14/14 [00:08<00:00,  1.72it/s]\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Sat Feb 13 16:30:43 2021 \n",
      "\n",
      "    Fold:1, Epoch:6, LR:0.0009645834, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                6.0863   |   6.6991\n",
      "\n",
      "    LWLRAP:              0.6995   |   0.6828\n",
      "\n",
      "    Class Loss:          4.7192   |   5.0322\n",
      "\n",
      "    Consistency Loss:    1.3671   |   1.6669\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:7 - Loss:4.6191:   0%|          | 0/56 [00:01<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "def train(df, fold, writer=None):\n",
    "    train_df = df[df.fold != fold]\n",
    "    val_df = df[df.fold == fold]\n",
    "    train_loader = get_data_loader(train_df)\n",
    "    val_loader = get_data_loader(val_df)\n",
    "\n",
    "    student_model = get_model()\n",
    "    teacher_model = get_model(is_mean_teacher=True)\n",
    "    optimizer = torch.optim.AdamW(student_model.parameters(), lr=config.lr, weight_decay=0.01)\n",
    "    warmup_prob = 0.2\n",
    "    num_train_steps = int(len(train_loader) * config.epochs)\n",
    "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_train_steps)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, epochs=config.epochs, steps_per_epoch=len(train_loader), pct_start=warmup_prob, div_factor=25, anneal_strategy='cos', cycle_momentum=True)\n",
    "    criterion = MeanTeacherLoss().cuda()\n",
    "    best_val_metric = -np.inf\n",
    "    val_metrics = []\n",
    "    train_metrics = []\n",
    "    for epoch in range(0, config.epochs):\n",
    "        train_loss_metrics = train_one_epoch(\n",
    "            student_model, teacher_model, train_loader, \n",
    "            criterion, optimizer, scheduler, epoch)\n",
    "        val_loss_metrics = train_one_epoch(\n",
    "            student_model, teacher_model, val_loader, \n",
    "            criterion, optimizer, scheduler, epoch, is_val=True)\n",
    "\n",
    "        train_metrics.append(train_loss_metrics)\n",
    "        val_metrics.append(val_loss_metrics)\n",
    "        pretty_print_metrics(fold, epoch, optimizer, \n",
    "                             train_loss_metrics, val_loss_metrics)\n",
    "        n_iters = len(train_loader) * (epoch + 1)\n",
    "        writer.add_scalar('fold_{}/train_loss'.format(fold), train_loss_metrics['loss'], n_iters)\n",
    "        writer.add_scalar('fold_{}/train_LWLRAP'.format(fold), train_loss_metrics['lwlrap'], n_iters)\n",
    "        writer.add_scalar('fold_{}/learning_rate'.format(fold), scheduler.get_last_lr()[0], n_iters)\n",
    "        writer.add_scalar('fold_{}/validate_loss'.format(fold), val_loss_metrics['loss'], n_iters)\n",
    "        writer.add_scalar('fold_{}/validate_LWLRAP'.format(fold), val_loss_metrics['lwlrap'], n_iters)\n",
    "        \n",
    "        if val_loss_metrics['lwlrap'] > best_val_metric:\n",
    "            print(f\"    LWLRAP Improved from {best_val_metric} --> {val_loss_metrics['lwlrap']}\\n\")\n",
    "            torch.save(teacher_model.state_dict(), os.path.join(config.save_path, f'{config.model_name}-fold-{fold}.bin'))\n",
    "            best_val_metric = val_loss_metrics['lwlrap']\n",
    "#     torch.save(teacher_model.state_dict(), os.path.join(config.save_path, f'{config.model_name}-fold-{fold}.bin'))\n",
    "    \n",
    "\n",
    "df = get_n_fold_df(config.train_tp_csv)\n",
    "time_stamp = '{0:%m_%d_%H_%M}'.format(datetime.datetime.now())\n",
    "writer=None\n",
    "writer = SummaryWriter(log_dir=os.path.join(config.tensorboard_path, '{}_{}'.format(config.model_name, time_stamp)))\n",
    "for fold in range(5 if config.train_5_folds else 1):\n",
    "    train(df, fold, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on Test Set\n",
    "We'll predict using the teacher model but you could also use the student or a combination of the two. Inference works just like it would for a vanilla baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-13T15:58:27.308Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(test_df, train_fold):\n",
    "    test_dataset = TestDataset(\n",
    "        df=test_df,\n",
    "        data_path=config.test_data_path,\n",
    "        period=config.period,\n",
    "        step=config.step\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=config.num_workers\n",
    "    )\n",
    "    \n",
    "    weights_path = os.path.join(config.save_path, f'{config.model_name}-fold-{fold}.bin')\n",
    "    model = get_model()\n",
    "    model.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu')), strict=False)\n",
    "    \n",
    "    test_pred, ids = predict_on_test(model, test_loader)\n",
    "\n",
    "    # Build Submission File\n",
    "    test_pred_df = pd.DataFrame({\n",
    "        \"recording_id\": test_df.recording_id.values\n",
    "    })\n",
    "    target_cols = test_df.columns[1:].values.tolist()\n",
    "    test_pred_df = test_pred_df.join(pd.DataFrame(np.array(test_pred), \n",
    "                                                  columns=target_cols))\n",
    "    test_pred_df.to_csv(os.path.join(config.save_path, \n",
    "                                     f\"{config.model_name}-fold-{train_fold}-submission.csv\"), \n",
    "                        index=False)\n",
    "    \n",
    "    \n",
    "test_df = pd.read_csv(config.test_csv)\n",
    "for fold in range(5 if config.train_5_folds else 1):\n",
    "    test(test_df, fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Fold Ensemble\n",
    "For 5 fold runs, we'll create a single ensemble prediction by simply averaging all of the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-13T15:58:28.127Z"
    }
   },
   "outputs": [],
   "source": [
    "def ensemble(submission_path):\n",
    "    dfs = [pd.read_csv(os.path.join(\n",
    "        config.save_path, f\"{config.model_name}-fold-{i}-submission.csv\")) for i in range(5)]\n",
    "    anchor = dfs[0].copy()\n",
    "    cols = anchor.columns[1:]\n",
    "    for c in cols:\n",
    "        total = 0\n",
    "        for df in dfs:\n",
    "            total += df[c]\n",
    "        anchor[c] = total / len(dfs)\n",
    "    anchor.to_csv(submission_path, index=False)\n",
    "\n",
    "\n",
    "submission_path = os.path.join(config.res_path, f\"{config.model_name}-submission.csv\")\n",
    "if config.train_5_folds:\n",
    "    ensemble(submission_path)\n",
    "else:\n",
    "    fold0_submission = os.path.join(config.res_path, f\"fold-0-submission.csv\")\n",
    "    os.rename(fold0_submission, submission_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "Thanks for reading! I dropped some unrelated tricks from this and didn't spend much time tuning so there's almost definetely room for improvement.\n",
    "\n",
    "I know it's pretty late in the competition for new notebooks, but considering that there are a few other public notebooks that score higher, I'm hoping this won't cause a significant shakeup. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
